[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimization",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "basics/first-definitions.html#first-definitions",
    "href": "basics/first-definitions.html#first-definitions",
    "title": "1  Review of Optimization basics",
    "section": "1.1 First definitions",
    "text": "1.1 First definitions\n\n1.1.1 Why we need Calculus\nSuppose you have a function f(x) and you want to find the value of x that makes f(x) as large as possible. This is called a maximum.\nOur issue is thus to find various forms of maxima and minima. We denote this problem as follows:\n\n\n\n\n\n\n\n\n\nNumber\nObjective Name\nMathematical expression\nKey elements\n\n\n\n\n(1)\nUnconstrained Maximization\n\\max_x f(x)\nThe only restriction on f is the nature of its admissible inputs \\mathbf{dom}(f)\n\n\n(2)\nFinding the global maximizer\nx^\\star \\gets \\argmax_x f(x)\nThere is generally a more restriction set of assumptions than in (1) to find the precise x^\\star for which f is minimized\n\n\n(3)\nFinding the a maximiser within a constraint set\n\\underset{x\\in\\mathcal{X}}{\\max f(x)}\nIn this case, there is no possibility for x to go outside of a predefined set \\mathcal{X}, this is very useful in real-life applications\n\n\n\nWhen is calculus useful and not useful ? Here’s a first reason it might or might not be useful: the domain \\mathcal{X} of the input variable, or (which is often the same), the domain \\mathbf{dom}(f) or our function.\nA pretty important part of the problem is what we call the domain of the function. This is the set of values that x can take on. For example, if f(x) = x^2, then the domain is the set of real numbers \\mathbb{R}.\nDomains change everything because, for example, if your domain \\mathbf{dom}(X) is discrete, then the image \\mathbf{im}_f(X) (also denoted f(\\mathbf{dom}(X))) is also discrete, and you just end up with the array-sorting problem, for which efficient algorithms like quicksort exist..\nThe derivative of a function f(x)\\colon \\mathbb{R}\\to\\mathbb{R} is a function f'(x) is just what everyone knows, that is the limit of\nf'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h}\nWhen you have a function of several variables, say f(x_1, x_2, \\dots, x_n), whose values are still real numbers, then you can still behave as if nothing happened, because if you fix the n-1 variables\n\\bar{x}_{\\scriptscriptstyle-i} \\coloneqq (\\bar{x}_1,\\ldots,\\bar{x}_{\\scriptscriptstyle i-1},x_{\\scriptscriptstyle i+1},\\dots\\bar{x}_n)\nThen the univariate function g(x_i) is still a function of a single variable, and you can still take its derivative:\n g\\colon x \\mapsto f(\\bar{x}_{\\scriptscriptstyle-i}, x) \\coloneqq f(\\bar{x}_1,\\ldots,\\bar{x}_{\\scriptscriptstyle i-1},x_i, x_{\\scriptscriptstyle i+1},\\dots\\bar{x}_n)\nWe have a bunch of fancy notation for this, namely \\partial_i f(\\bar{x}) for short, or \\frac{\\partial f}{\\partial x_i}(\\bar{x}). We also sometimes speak of the directional derivative of f in the direction of \\bar{x}, which is often denoted D_{\\bar{x}}f.\nWhat is not clear though, is how we relate each derivative to the others. How does \\partial_i f(\\bar{x}) relate to \\partial_j f(\\bar{x})?\nThere is a good theorem to explain this !\n\n\n1.1.2 The Jacobian\nThe theorem that defines and cements the derivatives is the following:\n\nLet f\\colon \\mathbb{R}^n \\to \\mathbb{R}^m be a function. Then, at each point x where f is differentiable, there is a unique matrix M(x) so that:  f(x+h) \\approx f(x) + M(x)h +\\varepsilon (h) where \\varepsilon (h) is a small vector in \\mathbb{R}^m that goes to 0 faster than h does1.\n\n\n\n1.1.3 The Proof\nThe uniqueness is fairly easy to prove, because if there are two matrices M_1 and M_2 such that f(x+h) \\approx f(x) + M_1h and f(x+h) \\approx f(x) + M_2h, then we can subtract the two equations, (M_1 - M_2)h = \\varepsilon_1 (h) + \\varepsilon_2 (h)\ndivide by \\lvert{h}\\rvert to get that the LHS tends to M_1 - M_2 as h\\to 0, and the RHS tends to 0 as h\\to 0, so we get that M_1 - M_2 = 0.\nThe existence is essentially the notion that we take matrices as rectangular arrays of numbers. In that sense, if the matrix M(x) has coefficients \\left(\\smash{\\alpha_{ij}(x)}\\right)_{i,j}, then we can write, our equation is merely a statement that is 1-D (which we know how to deal with):\n\\begin{aligned}\nf(x+h) &= f(x) + M(x)h +\\varepsilon (h) &\\\\\n\\iff f_j(x+h) &= f_j(x) + \\sum_{i=1}^m \\alpha_{ij}(x)h_j + \\varepsilon_j (h) &\\forall j\\in\\{1,\\dots m\\} \\\\\n\\iff f_j(x_i+h_i) &= f_j(x_i) + \\sum_{i=1}^m \\alpha_{ij}(x_i)h_j + \\varepsilon_j (h_i) &\\forall j\\in\\{1,\\dots m\\},\\,\\forall i\\in\\{1,\\dots n\\} \\\\\n\\end{aligned}\nThis last set of equation is just a m\\times n grid of first order approximations for each f_j at each x_i, and we can solve for the \\alpha_{ij} by just taking the limit as h_i\\to 0 for each i2.\nIf you hadn’t guessed before, each of these equations are looking very much like \\varphi(x+h) \\approx \\varphi(x) + \\varphi'(x)h + \\varepsilon(h)\nwe know in 1D, and we can just take the limit as h\\to 0 to get that \\varphi'(x) = \\alpha_{ij}(x).\nWhat this proves is 3 things:\n\nThe Jacobian is a matrix, and it is unique.\nThe Jacobian is a linear operator, and it is continuous.\nThe Jacobian matrix is populated by the partial derivatives of f, that is\n\n \\textbf{Jac}[f](x) = \\left[\\dfrac{\\partial f_j}{\\partial x_i}(x)\\right]_{\\substack{1\\leq j \\leq m\\\\1\\leq j \\leq n}}\n\n\n\n\n\n\nCareful about the dimensions !\n\n\n\nThe jacobian sends points from a n-dimensional space to a m-dimensional space, so the Jacobian matrix is a m\\times n matrix. This is why the partial derivatives are written in the order \\dfrac{\\partial f_j}{\\partial x_i}, because the jth row of the matrix is the partial derivatives of f_j with respect to the x_i’s.\nThis is very counterintuitive to anyone who works with the gradient \\nabla f(x), which is a n-dimensional vector, and is the transpose of the Jacobian matrix.\n\n\n\n\n1.1.4 A small example using python\nLet’s take a look at a small example I made using python’s sympy library. Through graphing increasingly complex 2D functions on an online tool, I found a function that is complex enough for us to have fun, but not too complex that we can’t understand it.\nThe function is:\nG(x,y) = \\log\\left[1+\\left(x^{\\frac{1}{3}} - y^{\\frac{2}{3}}\\right)^2\\left(0.1+\\sin \\left(x\\right)^2\\cdot \\cos \\left(x\\right)^2\\right)^{-1}\\right]\n\nBut I think it’s easier to understand if we write it as:\n G(x,y) = \\log\\left(1 + \\dfrac{A(x,y)^2}{\\gamma + B(x,y)^2}\\right)\nwith A(x,y) = x^{\\frac{1}{3}} - y^{\\frac{2}{3}} and B(x,y) = \\sin \\left(x\\right)\\cdot \\cos \\left(x\\right).\nWe can plot this function using plotly:\n\n\nCode\nimport plotly.graph_objects as go\nimport requests as rq\nimport json\nfrom pathlib import Path\n\ntemplate = json.loads(\n    Path(\"../plotlyTemplate.json\").read_text()\n)\n\n\n\ndata = rq.get(\"https://storage.googleapis.com/open.data.arnov.dev/static/plots/optimization/CostFunction2D.json\")\n\nfig = go.FigureWidget(data=data.json())\n\nfig.update_layout(\n    **template,\n)\n\nfig.show()\n\n\n\n                                                \nA 3D plot of the function G(x,y) = \\log\\left(1 + \\dfrac{A(x,y)^2}{\\gamma + B(x,y)^2}\\right)"
  }
]