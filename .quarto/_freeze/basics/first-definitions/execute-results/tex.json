{
  "hash": "68632337153cdcfd6e6ad89efb8439ea",
  "result": {
    "markdown": "---\ninclude-in-header: |\n    \\newcommand\\dom{\\mathop{\\mathrm{dom}}\\limits}\n    \\newcommand\\im{\\mathop{\\mathrm{im}}\\limits}\n    \\newcommand\\supp{\\mathop{\\mathrm{supp}}\\limits}\n    \\newcommand\\id{\\mathop{\\mathrm{id}}\\limits}\n    \\newcommand\\tr{\\mathop{\\mathrm{tr}}\\limits}\n    \\newcommand\\diag{\\mathop{\\mathrm{diag}}\\limits}\n    \\newcommand\\sign{\\mathop{\\mathrm{sign}}\\limits}\n    \\newcommand\\sgn{\\mathop{\\mathrm{sgn}}\\limits}\n    \\newcommand\\dist{\\mathop{\\mathrm{dist}}\\limits}\n    \\newcommand\\argmin{\\mathop{\\mathrm{argmin}}\\limits}\n    \\newcommand\\argmax{\\mathop{\\mathrm{argmax}}\\limits}\n    \\newcommand\\minimax[2]{\\mathop{\\mathrm{min}_{#1}\\mathrm{max}_{#2}}\\limits}\n    \\newcommand\\maximin[2]{\\mathop{\\mathrm{max}_{#1}\\mathrm{min}_{#2}}\\limits}\n---\n\n# Review of Optimization basics {.Review}\n\n## First definitions {.FirstDefs}\n\n### Why we need Calculus\n\nSuppose you have a function $f(x)$ and you want to find the value of $x$ that makes $f(x)$ as large as possible. This is called a *maximum*.  \n\nOur issue is thus to find various forms of maxima and minima. We denote this problem as follows:\n\n| Number | Objective Name | Mathematical expression | Key elements |\n| --- | --- | ----- | ------ |\n| $(1)$ | Unconstrained Maximization | $\\max_x f(x)$ | The only restriction on $f$ is the nature of its admissible inputs $\\mathbf{dom}(f)$\n| $(2)$ | Finding the global maximizer | $x^\\star \\gets \\argmax_x f(x)$ | There is generally a more restriction set of assumptions than in $(1)$ to find the precise $x^\\star$ for which $f$ is minimized |\n| $(3)$ | Finding the a maximiser within a constraint set | $\\underset{x\\in\\mathcal{X}}{\\max f(x)}$ | In this case, there is no possibility for $x$ to go outside of a predefined set $\\mathcal{X}$, this is very useful in real-life applications |\n\nWhen is calculus useful and not useful ? Here's a first reason it might or might not be useful: the __domain__ $\\mathcal{X}$ of the input variable, or (which is often the same), the domain $\\mathbf{dom}(f)$ or our function.\n\n\nA pretty important part of the problem is what we call the *domain* of the function. This is the set of values that $x$ can take on. For example, if $f(x) = x^2$, then the domain is the set of real numbers $\\mathbb{R}$.  \n\n\n\nDomains change everything because, for example, if your domain $\\mathbf{dom}(X)$ is discrete, then the image $\\mathbf{im}_f(X)$ (also denoted $f(\\mathbf{dom}(X))$) is also discrete, and you just end up with the array-sorting problem, [for which efficient algorithms like quicksort exist.](https://en.wikipedia.org/wiki/Quicksort).\n\n\n**The derivative** of a function $f(x)\\colon \\mathbb{R}\\to\\mathbb{R}$ is a function $f'(x)$ is just what everyone knows, that is the limit of \n\n$$f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h}$$\n\n\n\nWhen you have a function of several variables, say $f(x_1, x_2, \\dots, x_n)$, whose values are still real numbers, then you can still behave as if nothing happened, because if you fix the $n-1$ variables\n\n$$\\bar{x}_{\\scriptscriptstyle-i} \\coloneqq (\\bar{x}_1,\\ldots,\\bar{x}_{\\scriptscriptstyle i-1},x_{\\scriptscriptstyle i+1},\\dots\\bar{x}_n)$$\n\nThen the **univariate function** $g(x_i)$ is still a function of a single variable, and you can still take its derivative:\n\n$$ g\\colon x \\mapsto f(\\bar{x}_{\\scriptscriptstyle-i}, x) \\coloneqq f(\\bar{x}_1,\\ldots,\\bar{x}_{\\scriptscriptstyle i-1},x_i, x_{\\scriptscriptstyle i+1},\\dots\\bar{x}_n)$$\n\n\n\nWe have a bunch of fancy notation for this, namely $\\partial_i f(\\bar{x})$ for short, or $\\frac{\\partial f}{\\partial x_i}(\\bar{x})$.\nWe also sometimes speak of the **directional derivative** of $f$ in the direction of $\\bar{x}$, which is often denoted $D_{\\bar{x}}f$.\n\nWhat is not clear though, is how we relate each derivative to the others. How does $\\partial_i f(\\bar{x})$ relate to $\\partial_j f(\\bar{x})$?\n\nThere is a good theorem to explain this !\n\n\n\n### The Jacobian\n\nThe theorem that defines and cements the derivatives is the following:\n\n> Let $f\\colon \\mathbb{R}^n \\to \\mathbb{R}^m$ be a function. Then, at each point $x$ where $f$ is differentiable, there is a **unique matrix** $M(x)$ so that:\n> $$ f(x+h) \\approx f(x) + M(x)h +\\varepsilon (h)$$\n> where $\\varepsilon (h)$ is a small vector in $\\mathbb{R}^m$ that goes to $0$ faster than $h$ does[^1].\n\n[^1]: in the sense of a norm: $\\lvert x\\rvert$, $\\varepsilon(h)/\\lvert x\\rvert\\to 0$ as $\\lvert x\\rvert\\to 0$.\n\n\n### The Proof\n\nThe uniqueness is fairly easy to prove, because if there are two matrices $M_1$ and $M_2$ such that $f(x+h) \\approx f(x) + M_1h$ and $f(x+h) \\approx f(x) + M_2h$, then we can subtract the two equations,\n$$(M_1 - M_2)h = \\varepsilon_1 (h) + \\varepsilon_2 (h)$$\n\ndivide by $\\lvert{h}\\rvert$ to get that the LHS tends to $M_1 - M_2$ as $h\\to 0$, and the RHS tends to $0$ as $h\\to 0$, so we get that $M_1 - M_2 = 0$.\n\nThe existence is essentially the notion that we take matrices as rectangular arrays of numbers. In that sense, if the matrix $M(x)$ has coefficients $\\left(\\smash{\\alpha_{ij}(x)}\\right)_{i,j}$, then we can write, our equation is merely a statement that is 1-D (which we know how to deal with):\n\n\\begin{aligned}\nf(x+h) &= f(x) + M(x)h +\\varepsilon (h) &\\\\\n\\iff f_j(x+h) &= f_j(x) + \\sum_{i=1}^m \\alpha_{ij}(x)h_j + \\varepsilon_j (h) &\\forall j\\in\\{1,\\dots m\\} \\\\\n\\iff f_j(x_i+h_i) &= f_j(x_i) + \\sum_{i=1}^m \\alpha_{ij}(x_i)h_j + \\varepsilon_j (h_i) &\\forall j\\in\\{1,\\dots m\\},\\,\\forall i\\in\\{1,\\dots n\\} \\\\\n\\end{aligned}\n\nThis last set of equation is just a $m\\times n$ grid of first order approximations for each $f_j$ at each $x_i$, and we can solve for the $\\alpha_{ij}$ by just taking the limit as $h_i\\to 0$ for each $i$[^2]. \n\nIf you hadn't guessed before, each of these equations are looking very much like \n$$\\varphi(x+h) \\approx \\varphi(x) + \\varphi'(x)h + \\varepsilon(h)$$\n\n we know in 1D, and we can just take the limit as $h\\to 0$ to get that $\\varphi'(x) = \\alpha_{ij}(x)$.  \n\nWhat this proves is 3 things:\n\n1. The Jacobian is a matrix, and it is unique.\n2. The Jacobian is a linear operator, and it is continuous.\n3. The Jacobian matrix is populated by the partial derivatives of $f$, that is\n\n$$ \\textbf{Jac}[f](x) = \\left[\\dfrac{\\partial f_j}{\\partial x_i}(x)\\right]_{\\substack{1\\leq j \\leq m\\\\1\\leq j \\leq n}}$$\n\n\n::: {.callout-caution}\n## Careful about the dimensions !\nThe jacobian sends points from a $n$-dimensional space to a $m$-dimensional space, so the Jacobian matrix is a $m\\times n$ matrix. This is why the partial derivatives are written in the order $\\dfrac{\\partial f_j}{\\partial x_i}$, because the $j$th row of the matrix is the partial derivatives of $f_j$ with respect to the $x_i$'s.  \n\nThis is very counterintuitive to anyone who works with the gradient $\\nabla f(x)$, which is a $n$-dimensional vector, and is the transpose of the Jacobian matrix.\n:::\n\n### A small example using python\n\nLet's take a look at a small example I made using python's `sympy` library. Through [graphing increasingly complex 2D functions on an online tool](https://www.math3d.org/), I found a function that is complex enough for us to have fun, but not too complex that we can't understand it.\n\nThe function is:\n\n\n$$G(x,y) = \\log\\left[1+\\left(x^{\\frac{1}{3}} - y^{\\frac{2}{3}}\\right)^2\\left(0.1+\\sin \\left(x\\right)^2\\cdot \\cos \\left(x\\right)^2\\right)^{-1}\\right]\n$$\n\nBut I think it's easier to understand if we write it as:\n\n$$ G(x,y) = \\log\\left(1 + \\dfrac{A(x,y)^2}{\\gamma + B(x,y)^2}\\right)$$\n\nwith $A(x,y) = x^{\\frac{1}{3}} - y^{\\frac{2}{3}}$ and $B(x,y) = \\sin \\left(x\\right)\\cdot \\cos \\left(x\\right)$.\n\nWe can plot this function using `plotly`:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport plotly.graph_objects as go\nimport requests as rq\n\ndata = rq.get(\"https://storage.googleapis.com/open.data.arnov.dev/static/plots/optimization/CostFunction2D.json\")\n\nfig = go.FigureWidget(data=data.json())\n\nfig.update_layout(\n    paper_bgcolor=\"rgba(177, 145, 231, 0.3)\", plot_bgcolor=\"rgba(121, 78, 193, 0.48)\", modebar=dict(bgcolor=\"rgba(177, 145, 231, 0.3)\"),\n)\n\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n\nA 3D plot of the function $G(x,y) = \\log\\left(1 + \\dfrac{A(x,y)^2}{\\gamma + B(x,y)^2}\\right)$\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n:::\n\n\n[^2]: There is a subtelty here, because we can't really just let any $h_i$ to zero before the others arbitrarily, but if we do it in any order according to most used norms (here we'll use the euclidean norm $\\lvert \\cdot \\rvert$), then we get that the limit exists and is unique.)\t\n\n",
    "supporting": [
      "first-definitions_files\\figure-pdf"
    ],
    "filters": []
  }
}